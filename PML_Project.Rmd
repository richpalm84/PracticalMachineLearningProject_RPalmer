---
title: "PML - Course Project - Predicting Exercise"
author: "Richard Palmer"
date: "April 6, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Predicting if a weight lifting exercise was done correctly

This report describes how I built my model, used cross validation, what I think the expected out of sample error is, and why I made the choices I did.  

###Acknowledgments

Data for this model came from: http://groupware.les.inf.puc-rio.br/har. 
Here is the source paper: 
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

##Building my model
I first read in my data and looked at it to determine if there were any variables that I could readily discard. As part of the data read-in process I split the data into a training set (70%) and testing test (30%) for cross validation.  

```{r, echo=FALSE}
library(caret)
library(dplyr)

library(rattle)   
library(magrittr) # For the %>% and %<>% pipeline operators.

set.seed(42) 

#============================================================

# Load the training dataset from file.

fname <- "file:///C:/Users/Palmer/Documents/Richy's Docs/Data Science Course/PracticalMachineLearningProject/pml-training.csv" 
dataset <- read.csv(fname,
			na.strings=c(".", "NA", "", "?"),
			strip.white=TRUE, encoding="UTF-8")

nobs     <- nrow(dataset)
train    <- sample <- sample(nobs, 0.7*nobs)
validate <- NULL
test     <- setdiff(setdiff(seq_len(nobs), train), validate)
```

After examining the data I noted that there were several date/time and identifier (eg name of participant) variables that I would exclude.  After further testing, I also found that there were variables that had mostly NA values.  I excluded these variables from my model as well.  

```{r}
#============================================================

# The following variable selections have been noted.

input     <- c("roll_belt", "pitch_belt", "yaw_belt",
                   "total_accel_belt", 
                   "gyros_belt_x", "gyros_belt_y", "gyros_belt_z",
                   "accel_belt_x", "accel_belt_y", "accel_belt_z",
                   "magnet_belt_x", "magnet_belt_y", "magnet_belt_z",
                   "roll_arm", "pitch_arm", "yaw_arm", "total_accel_arm",
                   "gyros_arm_x", "gyros_arm_y", "gyros_arm_z",
                   "accel_arm_x", "accel_arm_y", "accel_arm_z", "magnet_arm_x",
                   "magnet_arm_y", "magnet_arm_z", "roll_dumbbell",
                   "pitch_dumbbell", "yaw_dumbbell", "total_accel_dumbbell",
                   "gyros_dumbbell_x", "gyros_dumbbell_y", "gyros_dumbbell_z",
                   "accel_dumbbell_x", "accel_dumbbell_y", "accel_dumbbell_z",
                   "magnet_dumbbell_x", "magnet_dumbbell_y",
                   "magnet_dumbbell_z", "roll_forearm", "pitch_forearm",
                   "yaw_forearm", "total_accel_forearm", "gyros_forearm_x", "gyros_forearm_y", "gyros_forearm_z",
                   "accel_forearm_x", "accel_forearm_y", "accel_forearm_z",
                   "magnet_forearm_x", "magnet_forearm_y", "magnet_forearm_z")

target    <- "classe"

```

##Which method should I use?

I used 4 different methods to see which one would predict with the highest accuracy. 
Here are the accuracy rates for each:

*Decision Tree: 69.4%
*Random Forest: 95.9%
*SVM: 93.5%
*Linear: 74.4%

Based on this, I chose to use the Random Forest method for my final model.

```{r}
#============================================================
# Build a Random Forest model using the traditional approach.

set.seed(42)

rf <- randomForest::randomForest(classe ~ .,
  data=dataset[, c(input, target)], 
  ntree=500,
  mtry=10,
  sampsize=c(1000),
  importance=TRUE,
  na.action=randomForest::na.roughfix,
  replace=FALSE)

#============================================
```

```{r, eval=FALSE}
# Decision Tree 

# The 'rpart' package provides the 'rpart' function.

library(rpart, quietly=TRUE)

set.seed(42)

# Build the Decision Tree model.

rpart <- rpart(classe ~ .,
    data=dataset[train, c(input, target)],
    method="class",
    parms=list(split="information"),
    control=rpart.control(usesurrogate=0, 
        maxsurrogate=0))

# Generate a textual view of the Decision Tree model.

print(rpart)
printcp(rpart)
cat("\n")

#============================================================
# Support vector machine. 
# The 'kernlab' package provides the 'ksvm' function.

library(kernlab, quietly=TRUE)

# Build a Support Vector Machine model.
set.seed(42)
crs$ksvm <- ksvm(as.factor(classe) ~ .,
      data=dataset[train,c(input, target)],
      kernel="rbfdot",
      prob.model=TRUE)

# Generate a textual view of the SVM model.

crs$ksvm

#============================================================
# Regression model 
# Build a multinomial model using the nnet package.
library(nnet, quietly=TRUE)
# Summarise multinomial model using Anova from the car package.
library(car, quietly=TRUE)
# Build a Regression model.

glm <- multinom(classe ~ ., data=dataset[train,c(input, target)], trace=FALSE, maxit=1000)

# Generate a textual view of the Linear model.

rattle.print.summary.multinom(summary(glm,
                              Wald.ratios=TRUE))
cat(sprintf("Log likelihood: %.3f (%d df)
", logLik(glm)[1], attr(logLik(glm), "df")))
if (is.null(glm$na.action)) omitted <- TRUE else omitted <- -glm$na.action
cat(sprintf("Pseudo R-Square: %.8f

",cor(apply(glm$fitted.values, 1, function(x) which(x == max(x))),
as.integer(dataset[train,][omitted,]$classe))))

cat('==== ANOVA ====
')
print(Anova(glm))
print("
")
```

###Here are the summary results running the RF model on my training data.

```{r, echo=FALSE}
rf

# List the importance of the variables.

#rn <- round(randomForest::importance(rf), 2)
#rn[order(rn[,3], decreasing=TRUE),]
```

###Running my model on the test set for cross validation

I then ran the model on my test set and found the following results:

```{r, echo=FALSE}
# Generate an Error Matrix for the Random Forest model.

# Obtain the response from the Random Forest model.
set.seed(42)
pr <- predict(rf, newdata=na.omit(dataset[test, c(input, target)]))

# Generate the confusion matrix showing counts.

rattle::errorMatrix(na.omit(dataset[test, c(input, target)])$classe, pr, count=TRUE)

# Generate the confusion matrix showing proportions.

(per <- rattle::errorMatrix(na.omit(dataset[test, c(input, target)])$classe, pr))

# Calculate the overall error percentage.

cat(100-sum(diag(per), na.rm=TRUE))

# Calculate the averaged class error percentage.

#cat(mean(per[,"Error"], na.rm=TRUE))
```

##Out of sample error rate
The number reported above is the overall error percentage.  This shows that my accuracy actually increased.  This would mean that my out of sample error rate is 97.2%

##Why I made the choices I did

In selecting variables I tried to use common sense in selecting variables.  I would have used the average/std deviaion/max/min/var variables but there were so many missing values I decided to remove these variables.  

I selected my model based on the accuracy of it.  

```{r, eval=FALSE}
#==========================================================
#Load in test data for prediction for quiz

fname <- "file:///C:/Users/Palmer/Documents/Richy's Docs/Data Science Course/PracticalMachineLearningProject/pml-testing.csv" 
datatest <- read.csv(fname,
                        na.strings=c(".", "NA", "", "?"),
                        strip.white=TRUE, encoding="UTF-8")

#==========================================================
datatest1 <- datatest[,c(input)]
predRF <- predict(rf, datatest1)

output <- cbind(datatest, predRF)

```







